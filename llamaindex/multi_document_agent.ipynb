{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b523e0a",
   "metadata": {},
   "source": [
    "# Building a Multi-Document Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a323703",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9625ab2-71b6-4fd0-904e-42df80d3215f",
   "metadata": {
    "height": 46,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20adaa26",
   "metadata": {},
   "source": [
    "## 1. Setup an agent over 3 papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed10a24b-d65c-4b98-a93a-94ccdb8900d0",
   "metadata": {
    "height": 216,
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d8f3185-3221-4b00-bd38-41d36e4a3307",
   "metadata": {
    "height": 165,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: loftq.pdf\n",
      "Getting tools for paper: swebench.pdf\n",
      "Getting tools for paper: selfrag.pdf\n",
      "Getting tools for paper: zipformer.pdf\n",
      "Getting tools for paper: values.pdf\n",
      "Getting tools for paper: finetune_fair_diffusion.pdf\n",
      "Getting tools for paper: knowledge_card.pdf\n",
      "Getting tools for paper: metra.pdf\n",
      "Getting tools for paper: vr_mcl.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e541bdd-14e1-41b6-81b5-b1bfda078d07",
   "metadata": {
    "height": 46,
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3fd5478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<llama_index.core.tools.function_tool.FunctionTool object at 0x000002084407DCD0>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x00000208440AAB40>, <llama_index.core.tools.function_tool.FunctionTool object at 0x00000208440AFAD0>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x00000208440AC410>, <llama_index.core.tools.function_tool.FunctionTool object at 0x000002084128D3D0>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x0000020844082420>, <llama_index.core.tools.function_tool.FunctionTool object at 0x0000020842BBEFC0>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x000002084087E150>, <llama_index.core.tools.function_tool.FunctionTool object at 0x00000208440AF920>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x0000020844062A50>, <llama_index.core.tools.function_tool.FunctionTool object at 0x0000020843F24B60>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x0000020843F068D0>, <llama_index.core.tools.function_tool.FunctionTool object at 0x0000020840874200>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x0000020843F137A0>, <llama_index.core.tools.function_tool.FunctionTool object at 0x00000208440A8680>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x000002084A925B20>, <llama_index.core.tools.function_tool.FunctionTool object at 0x0000020844098EF0>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x00000208440AA750>, <llama_index.core.tools.function_tool.FunctionTool object at 0x000002084A91F350>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x000002084A91DC40>, <llama_index.core.tools.function_tool.FunctionTool object at 0x000002084AD66BD0>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x0000020843F2BD10>]\n"
     ]
    }
   ],
   "source": [
    "print(initial_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bff58c52",
   "metadata": {
    "height": 63
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f2c6a9f",
   "metadata": {
    "height": 29
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a124a438-5609-402e-8642-69d1088cb9ad",
   "metadata": {
    "height": 182,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17409d4c-05a9-4bf4-b74f-75135fa3cb6b",
   "metadata": {
    "height": 97,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset mentioned in the context is called SAFECONV. It is designed for research on conversational safety, annotating unsafe spans in utterances and providing safe alternative responses. The dataset contains unsafespans, unsafe responses, and safe alternative responses for over 100,000 dialogues from social media platforms. It aims to explain why an utterance is unsafe and offers guidance for generating safer responses. Comparisons with other datasets indicate that SAFECONV is more comprehensive and effective in identifying and mitigating unsafe behavior in chatbots.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"evaluation results\"}\n",
      "=== Function Output ===\n",
      "The evaluation results demonstrate that achieving better perplexity is associated with longer context sizes, indicating the effectiveness of the fine-tuning method. The models show improved perplexity as the context size increases, highlighting the success of extending the context window. Furthermore, the models are fine-tuned to handle longer documents by extending the position interpolation, maintaining moderate retrieval ability even with longer context lengths. The models were evaluated based on perplexity scores on test splits like PG19 and long-context benchmarks like LongBench and LEval, showcasing their performance compared to other models. Additionally, an efficiency analysis was conducted to compare the training hours and GPU memory costs of different fine-tuning methods.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in LongLoRA is called SAFECONV, which is designed for research on conversational safety. It contains unsafespans, unsafe responses, and safe alternative responses for over 100,000 dialogues from social media platforms. The dataset aims to identify and mitigate unsafe behavior in chatbots by providing guidance for generating safer responses.\n",
      "\n",
      "Regarding the evaluation results, the models in LongLoRA show improved perplexity with longer context sizes, indicating the effectiveness of the fine-tuning method. The models perform better as the context size increases, demonstrating the success of extending the context window. They are fine-tuned to handle longer documents by extending the position interpolation, maintaining moderate retrieval ability even with longer context lengths. The models were evaluated based on perplexity scores on test splits like PG19 and long-context benchmarks like LongBench and LEval, showcasing their performance compared to other models. Additionally, an efficiency analysis was conducted to compare the training hours and GPU memory costs of different fine-tuning methods.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
    "    \"and then tell me about the evaluation results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ace340b1-761f-4058-be41-68cf131541e4",
   "metadata": {
    "height": 63,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n",
      "=== Function Output ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of large language models by training them to retrieve, generate, and critique text passages and their own generation using reflection tokens. It allows for customization of model behaviors at test time and has shown significant improvements in model performance and factuality compared to other models. The system focuses on browser-assisted question-answering and aims to provide accurate and informative responses based on given instructions and evidence, evaluating relevance, supportiveness, and overall utility of the response.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is a framework that consists of the Action Units Relation Transformer (ART) and the Tampered AU Prediction (TAP). The ART component focuses on modeling relations between different facial action units at AU-agnostic patches to assist in forgery detection, while the TAP process alters AU-related regions to offer local tampering supervision, thereby improving the model's capability to handle unseen manipulation methods. The contributions of LongLoRA include achieving top-tier performance in cross-dataset and cross-manipulation evaluations, creating challenging pseudo-samples for model learning, and presenting qualitative visualizations of tampered regions.\n",
      "=== LLM Response ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of large language models by training them to retrieve, generate, and critique text passages and their own generation using reflection tokens. It allows for customization of model behaviors at test time and has shown significant improvements in model performance and factuality compared to other models. The system focuses on browser-assisted question-answering and aims to provide accurate and informative responses based on given instructions and evidence, evaluating relevance, supportiveness, and overall utility of the response.\n",
      "\n",
      "LongLoRA is a framework that consists of the Action Units Relation Transformer (ART) and the Tampered AU Prediction (TAP). The ART component focuses on modeling relations between different facial action units at AU-agnostic patches to assist in forgery detection, while the TAP process alters AU-related regions to offer local tampering supervision, thereby improving the model's capability to handle unseen manipulation methods. The contributions of LongLoRA include achieving top-tier performance in cross-dataset and cross-manipulation evaluations, creating challenging pseudo-samples for model learning, and presenting qualitative visualizations of tampered regions.\n",
      "Self-RAG is a framework that enhances the quality and factuality of large language models by training them to retrieve, generate, and critique text passages and their own generation using reflection tokens. It allows for customization of model behaviors at test time and has shown significant improvements in model performance and factuality compared to other models. The system focuses on browser-assisted question-answering and aims to provide accurate and informative responses based on given instructions and evidence, evaluating relevance, supportiveness, and overall utility of the response.\n",
      "\n",
      "LongLoRA is a framework that consists of the Action Units Relation Transformer (ART) and the Tampered AU Prediction (TAP). The ART component focuses on modeling relations between different facial action units at AU-agnostic patches to assist in forgery detection, while the TAP process alters AU-related regions to offer local tampering supervision, thereby improving the model's capability to handle unseen manipulation methods. The contributions of LongLoRA include achieving top-tier performance in cross-dataset and cross-manipulation evaluations, creating challenging pseudo-samples for model learning, and presenting qualitative visualizations of tampered regions.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b345f06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about MetaGPT, compare it with SWE-Bench and LongLoRA\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"MetaGPT\"}\n",
      "=== Function Output ===\n",
      "MetaGPT is a meta-programming framework that enhances problem-solving capabilities in multi-agent systems based on Large Language Models (LLMs). It incorporates Standardized Operating Procedures (SOPs) to streamline workflows and improve collaboration among agents. MetaGPT utilizes role specialization, structured communication interfaces, and executable feedback mechanisms to enhance code generation quality during runtime. In experiments, MetaGPT has shown state-of-the-art performance in various benchmarks, outperforming other frameworks in tasks like code generation and software development. It simplifies the process of transforming abstract requirements into detailed class and function designs through a specialized division of labor and standard operating procedures workflow. MetaGPT generates functional applications by breaking down projects into tasks, analyzing code files based on functionality, and assigning tasks to engineers. It also enables QA engineers to generate unit test code and review it for identifying and fixing bugs, ensuring high-quality software. Ultimately, MetaGPT enhances natural language programming capabilities, making programming more accessible and transparent while ensuring user data privacy and security.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_swebench with args: {\"input\": \"SWE-Bench\"}\n",
      "=== Function Output ===\n",
      "SWE-Bench is an evaluation framework and benchmark dataset specifically designed to assess the capabilities of language models in resolving real-world software engineering problems. It consists of task instances derived from GitHub issues and pull requests in popular Python repositories, focusing on complex code changes like bug fixes, feature additions, and regression issues. The framework tasks language models with editing a codebase to address an issue, and the proposed solutions are evaluated against real tests. SWE-Bench offers a realistic setting for evaluating language models in software engineering, providing unique challenges beyond traditional code generation benchmarks. The benchmark is continuously updatable and aims to push the boundaries of language model capabilities in practical software engineering tasks.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is an efficient fine-tuning approach that extends the context length of large language models with minimal accuracy compromise. It uses Shifted Sparse Attention (S2-Attn) to approximate the standard self-attention pattern during training. Additionally, LongLoRA includes the Action Units Relation Transformer (ART) for modeling relations between facial action units and the Tampered AU Prediction (TAP) for tampering AU-related regions to improve model generalization. This method allows for extending the context length of models like Llama2 7B to 100k and 70B models to 32k on a single 8×A100 machine. It achieves state-of-the-art performance on cross-dataset and cross-manipulation evaluations, with the ART encoder capturing intra-face relations and the TAP process generating challenging pseudosamples for model learning.\n",
      "=== LLM Response ===\n",
      "MetaGPT is a meta-programming framework that enhances problem-solving capabilities in multi-agent systems based on Large Language Models (LLMs). It incorporates Standardized Operating Procedures (SOPs) to streamline workflows and improve collaboration among agents. MetaGPT utilizes role specialization, structured communication interfaces, and executable feedback mechanisms to enhance code generation quality during runtime. In experiments, MetaGPT has shown state-of-the-art performance in various benchmarks, outperforming other frameworks in tasks like code generation and software development. It simplifies the process of transforming abstract requirements into detailed class and function designs through a specialized division of labor and standard operating procedures workflow. MetaGPT generates functional applications by breaking down projects into tasks, analyzing code files based on functionality, and assigning tasks to engineers. It also enables QA engineers to generate unit test code and review it for identifying and fixing bugs, ensuring high-quality software. Ultimately, MetaGPT enhances natural language programming capabilities, making programming more accessible and transparent while ensuring user data privacy and security.\n",
      "\n",
      "SWE-Bench is an evaluation framework and benchmark dataset specifically designed to assess the capabilities of language models in resolving real-world software engineering problems. It consists of task instances derived from GitHub issues and pull requests in popular Python repositories, focusing on complex code changes like bug fixes, feature additions, and regression issues. The framework tasks language models with editing a codebase to address an issue, and the proposed solutions are evaluated against real tests. SWE-Bench offers a realistic setting for evaluating language models in software engineering, providing unique challenges beyond traditional code generation benchmarks. The benchmark is continuously updatable and aims to push the boundaries of language model capabilities in practical software engineering tasks.\n",
      "\n",
      "LongLoRA is an efficient fine-tuning approach that extends the context length of large language models with minimal accuracy compromise. It uses Shifted Sparse Attention (S2-Attn) to approximate the standard self-attention pattern during training. Additionally, LongLoRA includes the Action Units Relation Transformer (ART) for modeling relations between facial action units and the Tampered AU Prediction (TAP) for tampering AU-related regions to improve model generalization. This method allows for extending the context length of models like Llama2 7B to 100k and 70B models to 32k on a single 8×A100 machine. It achieves state-of-the-art performance on cross-dataset and cross-manipulation evaluations, with the ART encoder capturing intra-face relations and the TAP process generating challenging pseudosamples for model learning.\n",
      "MetaGPT is a meta-programming framework that enhances problem-solving capabilities in multi-agent systems based on Large Language Models (LLMs). It incorporates Standardized Operating Procedures (SOPs) to streamline workflows and improve collaboration among agents. MetaGPT utilizes role specialization, structured communication interfaces, and executable feedback mechanisms to enhance code generation quality during runtime. In experiments, MetaGPT has shown state-of-the-art performance in various benchmarks, outperforming other frameworks in tasks like code generation and software development. It simplifies the process of transforming abstract requirements into detailed class and function designs through a specialized division of labor and standard operating procedures workflow. MetaGPT generates functional applications by breaking down projects into tasks, analyzing code files based on functionality, and assigning tasks to engineers. It also enables QA engineers to generate unit test code and review it for identifying and fixing bugs, ensuring high-quality software. Ultimately, MetaGPT enhances natural language programming capabilities, making programming more accessible and transparent while ensuring user data privacy and security.\n",
      "\n",
      "SWE-Bench is an evaluation framework and benchmark dataset specifically designed to assess the capabilities of language models in resolving real-world software engineering problems. It consists of task instances derived from GitHub issues and pull requests in popular Python repositories, focusing on complex code changes like bug fixes, feature additions, and regression issues. The framework tasks language models with editing a codebase to address an issue, and the proposed solutions are evaluated against real tests. SWE-Bench offers a realistic setting for evaluating language models in software engineering, providing unique challenges beyond traditional code generation benchmarks. The benchmark is continuously updatable and aims to push the boundaries of language model capabilities in practical software engineering tasks.\n",
      "\n",
      "LongLoRA is an efficient fine-tuning approach that extends the context length of large language models with minimal accuracy compromise. It uses Shifted Sparse Attention (S2-Attn) to approximate the standard self-attention pattern during training. Additionally, LongLoRA includes the Action Units Relation Transformer (ART) for modeling relations between facial action units and the Tampered AU Prediction (TAP) for tampering AU-related regions to improve model generalization. This method allows for extending the context length of models like Llama2 7B to 100k and 70B models to 32k on a single 8×A100 machine. It achieves state-of-the-art performance on cross-dataset and cross-manipulation evaluations, with the ART encoder capturing intra-face relations and the TAP process generating challenging pseudosamples for model learning.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about MetaGPT, compare it with SWE-Bench and LongLoRA\"\n",
    ")\n",
    "print(str(response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eede70c",
   "metadata": {},
   "source": [
    "## 2. Setup an agent over 11 papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18771e69",
   "metadata": {},
   "source": [
    "### Download 11 ICLR papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60d01d2c-547f-4054-b0fe-ed9b1a9cc3b5",
   "metadata": {
    "height": 488,
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
    "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
    "    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
    "    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
    "    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
    "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
    "    \"https://openreview.net/pdf?id=TpD2aG1h0D\"\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"loftq.pdf\",\n",
    "    \"swebench.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "    \"zipformer.pdf\",\n",
    "    \"values.pdf\",\n",
    "    \"finetune_fair_diffusion.pdf\",\n",
    "    \"knowledge_card.pdf\",\n",
    "    \"metra.pdf\",\n",
    "    \"vr_mcl.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea5ee34d-02ac-4537-ae20-7ef6c5767172",
   "metadata": {
    "height": 165,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: loftq.pdf\n",
      "Getting tools for paper: swebench.pdf\n",
      "Getting tools for paper: selfrag.pdf\n",
      "Getting tools for paper: zipformer.pdf\n",
      "Getting tools for paper: values.pdf\n",
      "Getting tools for paper: finetune_fair_diffusion.pdf\n",
      "Getting tools for paper: knowledge_card.pdf\n",
      "Getting tools for paper: metra.pdf\n",
      "Getting tools for paper: vr_mcl.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e35d52c",
   "metadata": {},
   "source": [
    "### Extend the Agent with Tool Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20154923-873e-4941-9a3a-4926ab5f9b8c",
   "metadata": {
    "height": 46,
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "671582f9-70d7-4a8f-b813-58b2a068ca72",
   "metadata": {
    "height": 165,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3929882-e9dc-46ca-b495-53e3ed60340e",
   "metadata": {
    "height": 46,
    "tags": []
   },
   "outputs": [],
   "source": [
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba9cfecd-fe14-4da8-b9ba-b3d485d98a03",
   "metadata": {
    "height": 80,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tools = obj_retriever.retrieve(\n",
    "    \"Tell me about the eval dataset used in MetaGPT and SWE-Bench\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c270ffbf-69c7-48ea-a028-9ba25221cde5",
   "metadata": {
    "height": 29,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='Use ONLY IF you want to get a holistic summary of MetaGPT. Do NOT use if you have specific questions over MetaGPT.', name='summary_tool_swebench', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[2].metadata\n",
    "tools[1].metadata\n",
    "tools[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9cc0a0b6-9858-4348-9ae0-1cd4160f3fb7",
   "metadata": {
    "height": 267,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tool_retriever=obj_retriever,\n",
    "    llm=llm, \n",
    "    system_prompt=\"\"\" \\\n",
    "You are an agent designed to answer queries over a set of given papers.\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "\"\"\",\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a250cf1a-e011-4994-bcca-4e0294f20864",
   "metadata": {
    "height": 114,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metra with args: {\"input\": \"evaluation dataset used in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in MetaGPT is not explicitly mentioned in the provided context information.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_swebench with args: {\"input\": \"evaluation dataset used in SWE-Bench\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in SWE-Bench consists of task instances constructed from pull requests that meet specific criteria, including being merged, resolving issues, and introducing new tests. Each task instance includes the codebase, problem statement, a test patch, and a gold patch. The dataset is validated through execution-based verification to ensure the usability and correctness of the solutions. It is designed to evaluate the performance of language models in generating code patches to address software engineering tasks and can be easily updated with new task instances based on PRs created after the training date of language models.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in MetaGPT is not explicitly mentioned in the provided context information. \n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench consists of task instances constructed from pull requests that meet specific criteria, including being merged, resolving issues, and introducing new tests. Each task instance includes the codebase, problem statement, a test patch, and a gold patch. The dataset is validated through execution-based verification to ensure the usability and correctness of the solutions. It is designed to evaluate the performance of language models in generating code patches to address software engineering tasks and can be easily updated with new task instances based on PRs created after the training date of language models.\n",
      "The evaluation dataset used in MetaGPT is not explicitly mentioned in the provided context information. \n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench consists of task instances constructed from pull requests that meet specific criteria, including being merged, resolving issues, and introducing new tests. Each task instance includes the codebase, problem statement, a test patch, and a gold patch. The dataset is validated through execution-based verification to ensure the usability and correctness of the solutions. It is designed to evaluate the performance of language models in generating code patches to address software engineering tasks and can be easily updated with new task instances based on PRs created after the training date of language models.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used \"\n",
    "    \"in MetaGPT and compare it against SWE-Bench\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8084c8cb-98ed-4835-aaa4-5b0c7254be6d",
   "metadata": {
    "height": 97,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Compare and contrast the LoRA papers (LongLoRA, LoftQ). Analyze the approach in each paper first. \n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA paper\"}\n",
      "=== Function Output ===\n",
      "The LongLoRA paper introduces an efficient fine-tuning approach to extend the context length of large language models, utilizing Shifted Sparse Attention (S2-Attn) during training to approximate standard self-attention patterns. It also presents an Action Units Relation Learning framework that includes the Action Units Relation Transformer (ART) and the Tampered AU Prediction (TAP) processes. The ART encoder focuses on modeling relations between facial action units at AU-agnostic patches for forgery detection, while the TAP process tampers AU-related regions to enhance generalization to unseen manipulation methods. The paper achieves state-of-the-art performance on cross-dataset and cross-manipulation evaluations, demonstrating the effectiveness and generalization of the proposed framework.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_loftq with args: {\"input\": \"LoftQ paper\"}\n",
      "=== Function Output ===\n",
      "The LoftQ paper introduces a novel quantization framework tailored for Large Language Models (LLMs), combining quantization and low-rank approximation techniques to enhance model compression. It demonstrates superior performance over existing quantization methods, particularly in challenging low-bit scenarios, across a range of natural language processing tasks. The paper also delves into implementation specifics, hyperparameter configurations, and comparisons with pruning methods, showcasing LoftQ's effectiveness in reducing memory usage and improving model generalization through Low-Rank Adaptation (LoRA) fine-tuning. Furthermore, the paper explores the extension of low-rank adapters to convolutional layers, illustrating the versatility of LoftQ across different neural network architectures.\n",
      "=== LLM Response ===\n",
      "The LongLoRA paper introduces an efficient fine-tuning approach to extend the context length of large language models. It utilizes Shifted Sparse Attention (S2-Attn) during training to approximate standard self-attention patterns. Additionally, it presents an Action Units Relation Learning framework that includes the Action Units Relation Transformer (ART) and the Tampered AU Prediction (TAP) processes. The ART encoder focuses on modeling relations between facial action units at AU-agnostic patches for forgery detection, while the TAP process tampers AU-related regions to enhance generalization to unseen manipulation methods. The paper achieves state-of-the-art performance on cross-dataset and cross-manipulation evaluations, demonstrating the effectiveness and generalization of the proposed framework.\n",
      "\n",
      "On the other hand, the LoftQ paper introduces a novel quantization framework tailored for Large Language Models (LLMs). It combines quantization and low-rank approximation techniques to enhance model compression. LoftQ demonstrates superior performance over existing quantization methods, particularly in challenging low-bit scenarios, across a range of natural language processing tasks. The paper also delves into implementation specifics, hyperparameter configurations, and comparisons with pruning methods, showcasing LoftQ's effectiveness in reducing memory usage and improving model generalization through Low-Rank Adaptation (LoRA) fine-tuning. Furthermore, the paper explores the extension of low-rank adapters to convolutional layers, illustrating the versatility of LoftQ across different neural network architectures.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Compare and contrast the LoRA papers (LongLoRA, LoftQ). \"\n",
    "    \"Analyze the approach in each paper first. \"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
